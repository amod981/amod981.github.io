How I Improved GPT-4o Classification in a RAG System by 42% Without Increasing Risk
When working with large language models in production, accuracy isn't just a quality metric ‚Äî it's the foundation for downstream decisions, especially in healthcare. I recently rewired how our patient response classification works within a RAG-based assistant used by clinics ‚Äî and boosted accuracy by 42% for knowledge base queries, all without adding clinical risk.

üß© The Problem
Our assistant (Flo) uses LLMs to interpret patient replies, then routes them through a tree-like logic system called floKB. This downstream logic assumes that the initial intent classification is reliable. But it wasn‚Äôt.

Patients often replied with vague, emotional, or indirect responses ‚Äî things like:

"Please stop!"

"I‚Äôve had a horrible day..."

"I already told my doctor."

The GPT-4o classification system misinterpreted these, triggering wrong flows, introducing clinical risk, and lowering end-to-end accuracy. A flat classification call was too brittle for the messy edges of human language.

üõ† My Solution: Drill-Down Classification with Consolidation
Rather than making one flat classification call to OpenAI, I redesigned the pipeline as a multi-stage system:

1. Level 1: High-Level Categorization
A simple first pass determines the broad category:

Reading

Knowledge base query

Acknowledgement

General

This level has fewer labels and clearer boundaries, which improves reliability.

2. Level 2: Targeted Subclassification
Based on the Level 1 result, I trigger a second call only within that subdomain:

A knowledge base query might become: assistance, reschedule, or question

A reading might be further refined into: measurement, keyword, etc.

Each Level 2 prompt is tightly scoped, which means:

Fewer distractions for the model

Less overlap between categories

More accurate and scalable behavior without needing tons of examples

3. Consolidation Logic
Finally, I combine the Level 1 and 2 results into a single, production-ready label. This unified output plugs back into our RAG system without any downstream change.

üìä The Results
After generating new labels using this multi-stage approach, I ran a full analysis comparing:

üß† Old (flat) labels

‚úÖ New consolidated labels

üéØ Expected/Gold labels

The improvements were clear:

42% increase in correctly identified knowledge base queries

12% boost in reading classification accuracy

**13% better acknowledgement detection

All with 0% increase in clinical risk

‚ö†Ô∏è Lessons Learned
GPT-4o struggles with niche boundaries ‚Äî even with good instructions. But narrowing scope dramatically improves results.

More focused prompts = more reliable behavior, even with fewer examples.

You don‚Äôt need fine-tuning if you design your calls and consolidation logic carefully.

‚úÖ Final Thoughts
The power of LLMs comes from their flexibility ‚Äî but reliability comes from structure. By switching from flat prompting to a layered classification pipeline, I made our assistant more robust, interpretable, and production-ready.

If you're building LLM-powered systems in noisy domains like healthcare, consider:

‚úÇÔ∏è Less generalization, more focus.

